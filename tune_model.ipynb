{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "688a7b05",
   "metadata": {},
   "source": [
    "## Model tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b5c113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize NumerAPI and Set Data Version\n",
    "from numerapi import NumerAPI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import lightgbm as lgb\n",
    "import cloudpickle\n",
    "import os\n",
    "import shutil\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "napi = NumerAPI()\n",
    "DATA_VERSION = \"v5.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0ed213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Download Data\n",
    "print(\"Downloading datasets...\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/train.parquet\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/features.json\")\n",
    "napi.download_dataset(f\"{DATA_VERSION}/validation.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cffc1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load Data\n",
    "print(\"Loading data...\")\n",
    "# Note: Requires 'pyarrow' installed in the .venv. Run `pip install pyarrow` if needed.\n",
    "feature_metadata = json.load(open(f\"{DATA_VERSION}/features.json\"))\n",
    "features = feature_metadata[\"feature_sets\"][\"medium\"] \n",
    "train = pd.read_parquet(f\"{DATA_VERSION}/train.parquet\", columns=[\"era\"] + features + [\"target\"])\n",
    "\n",
    "validation = pd.read_parquet(f\"{DATA_VERSION}/validation.parquet\", columns=[\"era\"] + features + [\"target\"])\n",
    "validation = validation[validation[\"target\"].notnull()]  # Filter rows with non-null targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc48c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Downsample for Speed\n",
    "print(\"Downsampling training data...\")\n",
    "train = train[train[\"era\"].isin(train[\"era\"].unique()[::4])]  # Skip every 4th era"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3ed767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Train Model\n",
    "print(\"Training model...\")\n",
    "model = lgb.LGBMRegressor(\n",
    "    n_estimators=20000,\n",
    "    learning_rate=0.001,\n",
    "    max_depth=8,\n",
    "    num_leaves=2**10-1,\n",
    "    colsample_bytree=0.1,\n",
    "    min_data_in_leaf=10000,\n",
    ")\n",
    "model.fit(\n",
    "    train[features],\n",
    "    train[\"target\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728fa8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Define reduce_mem_usage Function\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\"Optimize DataFrame memory usage by converting data types.\"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == \"float64\":\n",
    "            df[col] = df[col].astype(\"float32\")\n",
    "        if df[col].dtype == \"int64\":\n",
    "            df[col] = df[col].astype(\"int32\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516fddee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Load Live Data and Generate Predictions\n",
    "try:\n",
    "    print(\"Loading live data...\")\n",
    "    # Load live data, selecting only feature columns\n",
    "    live_data = pd.read_parquet(f\"{DATA_VERSION}/live.parquet\", columns=features)\n",
    "    print(f\"Live data columns: {list(live_data.columns)}\")\n",
    "    \n",
    "    # Optimize memory usage\n",
    "    live_data = reduce_mem_usage(live_data)\n",
    "    \n",
    "    # Generate predictions\n",
    "    live_predictions = model.predict(live_data[features])\n",
    "    # Create submission DataFrame with index as identifier\n",
    "    submission = pd.Series(live_predictions, index=live_data.index).to_frame(\"prediction\")\n",
    "    print(\"Predictions generated.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading live data or predicting: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538cedfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Save Predict Function and Verify Model File\n",
    "print(\"Saving predict function...\")\n",
    "def predict(live_features: pd.DataFrame, _live_benchmark_models: pd.DataFrame) -> pd.DataFrame:\n",
    "    live_predictions = model.predict(live_features[features])\n",
    "    submission = pd.Series(live_predictions, index=live_features.index)\n",
    "    return submission.to_frame(\"prediction\")\n",
    "\n",
    "# Pickle predict function\n",
    "model_file = \"lgbm_model.pkl\"\n",
    "with open(model_file, \"wb\") as f:\n",
    "    cloudpickle.dump(predict, f)\n",
    "\n",
    "# Verify the model file exists\n",
    "if os.path.exists(model_file):\n",
    "    print(f\"Model file '{model_file}' saved successfully in {os.getcwd()}.\")\n",
    "else:\n",
    "    raise FileNotFoundError(f\"Failed to save '{model_file}' in {os.getcwd()}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ad05b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Evaluate Model Locally\n",
    "print(\"Evaluating model on validation data...\")\n",
    "def compute_metrics(validation_data, features, model):\n",
    "    \"\"\"Compute Numerai metrics: CORR, FNC, Sharpe, Feature Exposure.\"\"\"\n",
    "    # Generate predictions\n",
    "    preds = model.predict(validation_data[features])\n",
    "    validation_data[\"prediction\"] = preds\n",
    "    \n",
    "    # CORR: Mean Spearman correlation per era\n",
    "    corrs = []\n",
    "    for era in validation_data[\"era\"].unique():\n",
    "        era_data = validation_data[validation_data[\"era\"] == era]\n",
    "        corr, _ = spearmanr(era_data[\"prediction\"], era_data[\"target\"])\n",
    "        corrs.append(corr)\n",
    "    mean_corr = np.mean(corrs)\n",
    "    sharpe = mean_corr / np.std(corrs) if np.std(corrs) > 0 else np.nan\n",
    "    \n",
    "    # Simplified FNC: Neutralize predictions against features\n",
    "    def neutralize_series(series, features_data):\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        X = features_data[features].fillna(0)\n",
    "        y = series.fillna(0)\n",
    "        reg = LinearRegression().fit(X, y)\n",
    "        neutralized = y - reg.predict(X)\n",
    "        return neutralized / neutralized.std() if neutralized.std() > 0 else neutralized\n",
    "    \n",
    "    neutralized_preds = neutralize_series(validation_data[\"prediction\"], validation_data)\n",
    "    fnc_corrs = []\n",
    "    for era in validation_data[\"era\"].unique():\n",
    "        era_data = validation_data[validation_data[\"era\"] == era]\n",
    "        corr, _ = spearmanr(neutralized_preds[era_data.index], era_data[\"target\"])\n",
    "        fnc_corrs.append(corr)\n",
    "    mean_fnc = np.mean(fnc_corrs)\n",
    "    \n",
    "    # Feature Exposure: Std of correlations between predictions and features\n",
    "    feature_corrs = [spearmanr(validation_data[\"prediction\"], validation_data[f])[0] for f in features]\n",
    "    feature_exposure = np.std(feature_corrs)\n",
    "    \n",
    "    return {\n",
    "        \"CORR\": mean_corr,\n",
    "        \"FNC\": mean_fnc,\n",
    "        \"Sharpe\": sharpe,\n",
    "        \"Feature Exposure\": feature_exposure\n",
    "    }\n",
    "\n",
    "# Compute and display metrics\n",
    "metrics = compute_metrics(validation, features, model)\n",
    "print(\"Local Validation Metrics:\")\n",
    "for metric, value in metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Optional: Save predictions for further analysis\n",
    "validation[\"prediction\"].to_csv(\"validation_predictions.csv\")\n",
    "print(f\"Validation predictions saved to 'validation_predictions.csv' in {os.getcwd()}.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
